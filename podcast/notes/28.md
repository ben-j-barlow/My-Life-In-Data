# Andrej Karpathy: Tesla AI, Self-Driving, Optimus, Aliens, and AGI

https://open.spotify.com/episode/28yFp9dzo61CiOf0Ejxxhe?si=344ba88318d64cf2,208

What is a neural net? Mathematical abstraction of brain. Simple expression. Dot products with non-linearities with knobs. Knobs inspired by brain synapses; modifiable, trainable.

What computation is biology doing that ANNs cannot? The optimisation that gives reason is performed very differently, therefore, I don’t draw comparisons. ANNs perform compression, BNN is an agent in a multi-agent self-play system that’s been running a very long time.

Books that explain origin of life isn’t that special: The Vital Question, A Life Ascending.

Given ease of origin of life, life is likely everywhere. We can’t see it because we can’t measure it, and it’s hard for them to travel to us.

Interstellar travel really hard: hydrogen atoms and dust particles have huge kinetic energy at that speed. Shielding, cosmic radiation protection, all very difficult.

Transformers

Biggest thing Andrej’s seen in AI and DL? Attention is all you need. Speech, text, video specific architectures were disposed of and converged to all use transformers. Is a general purpose differentiable computer that is
- Expressive: in forward psss
- Optimisable: in backward pass (backprop and GD)
- Efficient: highly parallelisable
Message passing scheme. Nodes have vectors. Messages “this is what I’m looking for” and other nodes broadcast “this is what I have”.

Because of residual pathway, in the backward pass, gradients flow uninterrupted - addition distributes the gradient equally to all of its branches.

Transformer in 2016 is the one you’d use today. Some minor changes in layer normalisation but major architecture working. Original model is very resilient. Transformer taking over AI.

Approach now: use a transformer and change everything else. Eg use a bigger dataset.

Using RL to train neural networks is very inefficient because rewards are so sparse. Gave neural networks access to keyboard and mouse. Too many options on a user interface. Starting with a GPT model, we have an understanding of interface elements (eg what is a ‘booking’, a ‘submit’?). Far more advantageous.

Software 2.0

Wrote an entire book about it
Weights instead of algorithms
Eg image classification: 
1. did everything algorithmically
2. built features, learned final layer
3. learn everything
This is happening everywhere. 
Should we have IDEs for weights? Best thing right now is HuggingFace.

Autopilot: gradually moving towards neural nets doing everything. Initially, neural net worked on each camera’s 2D image individually and C++ code fused everything together. Now, neural net acts on all cameras simultaneously, in a 4D world (3D world over time).

Approaches to collect data for training above network: Human annotation, simulation, Tesla’s offline 3D reconstruction process. Must find the spatial ground truth of objects to use in training.

Grew annotation team from 0 -> 1000.

“If you celebrate others, it actually makes you more successful.”

We’ve crushed MNIST and now ImageNet too.

Zebras can see and walk at birth. Evolution has found a way to code these neural networks into ACDGs. Mad!

Like Demis, more productive at night. Morning routine. Evening routine. Notices hard to ascend if routine is stressed, e.g., travelling. Worst thing in society: “I need 5 mins of your time”. The cost is always so much greater than 5 minutes.

Favourite IDE: VSCode. CoPilot integration is excellent. Learning curve to discover when it is helpful. Enables discovery of new APIs.

DeepMind will announce something large. Then we must wait 1 year for the details to be published in a prestigious journal. A shame. If details published immediately it would inspire the community more.

Beginners think too much about “what to do”. What’s more important is “how much you do”. Just pick a topic and get to work. Big believer in the 10,000 hours theory. Becoming an expert is a psychological problem. How can I build habits to reach 10,000 hours? Comparing to others is catastrophic.

Excellent at teaching. CS231 major part of teaching portfolio. If you have to explain something to someone else, you realise the gaps of your knowledge.

Diffusion models are 6 years old. Came from Google. Suddenly gained traction. Very successful. 

It’s difficult to go out into the physical world. We live our life in our brain anyway. The virtual world is so accessible. Humanity will head towards VR.

I want to solve all problems. If I solve the meta-problem (AI), I can use that to solve all other problems.

The power of uninterrupted work for momentum. Fasting experiments to determine productivity optimum.